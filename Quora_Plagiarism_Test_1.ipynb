{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora Plagiarism Test 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "womttD1TzUkB",
        "colab_type": "text"
      },
      "source": [
        "Experiment with Abishek Thakur's Quora plagiarism detector as an NTN testbed.\n",
        "[Article on Linkedin](https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG0E_LKwzQS8",
        "colab_type": "code",
        "outputId": "bec72e2f-cbb2-46d4-a0f9-6974b146f55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install -q keras==2.2.5\n",
        "![ -r is_that_a_duplicate_quora_question ] || git clone https://github.com/LanceNorskog/is_that_a_duplicate_quora_question.git\n",
        "!git pull\n",
        "%cd is_that_a_duplicate_quora_question"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 2.7MB/s \n",
            "\u001b[?25hfatal: not a git repository (or any of the parent directories): .git\n",
            "/content/is_that_a_duplicate_quora_question\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzBuItF9zddj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7fe7db2-b2ca-4d05-9e96-5c7bcdd5917c"
      },
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0wIo2G51Dof",
        "colab_type": "code",
        "outputId": "20b867fd-36bc-4be5-c6f2-051274e5ca4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!mkdir -p /tmp/data\n",
        "%cd /tmp/data\n",
        "![ -r glove.840B.300d.txt ] || wget -nc http://www-nlp.stanford.edu/data/glove.840B.300d.zip \n",
        "![ -r glove.840B.300d.txt ] || unzip glove.840B.300d.zip\n",
        "![ -r quora_duplicate_questions.tsv ] || wget -nc https://github.com/MLDroid/quora_duplicate_challenge/raw/master/data/quora_duplicate_questions.tsv\n",
        "![ -r GoogleNews-vectors-negative300.bin.gz ] || wget -nc https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!python -m nltk.downloader stopwords\n",
        "!ls\n",
        "%cd /content/is_that_a_duplicate_quora_question\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/data\n",
            "Archive:  glove.840B.300d.zip\n",
            "replace glove.840B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "glove.840B.300d.txt  GoogleNews-vectors-negative300.bin.gz\n",
            "glove.840B.300d.zip  quora_duplicate_questions.tsv\n",
            "/content/is_that_a_duplicate_quora_question\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI-NRXL34IKM",
        "colab_type": "code",
        "outputId": "4ca11806-9bba-4a4c-9865-3dfae97dc271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "!pwd\n",
        "!head /tmp/data/quora_duplicate_questions.tsv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/is_that_a_duplicate_quora_question\n",
            "id\tqid1\tqid2\tquestion1\tquestion2\tis_duplicate\n",
            "0\t1\t2\tWhat is the step by step guide to invest in share market in india?\tWhat is the step by step guide to invest in share market?\t0\n",
            "1\t3\t4\tWhat is the story of Kohinoor (Koh-i-Noor) Diamond?\tWhat would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\t0\n",
            "2\t5\t6\tHow can I increase the speed of my internet connection while using a VPN?\tHow can Internet speed be increased by hacking through DNS?\t0\n",
            "3\t7\t8\tWhy am I mentally very lonely? How can I solve it?\tFind the remainder when [math]23^{24}[/math] is divided by 24,23?\t0\n",
            "4\t9\t10\tWhich one dissolve in water quikly sugar, salt, methane and carbon di oxide?\tWhich fish would survive in salt water?\t0\n",
            "5\t11\t12\tAstrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\tI'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\t1\n",
            "6\t13\t14\tShould I buy tiago?\tWhat keeps childern active and far from phone and video games?\t0\n",
            "7\t15\t16\tHow can I be a good geologist?\tWhat should I do to be a great geologist?\t1\n",
            "8\t17\t18\tWhen do you use シ instead of し?\t\"When do you use \"\"&\"\" instead of \"\"and\"\"?\"\t0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNiQBwxp3C0x",
        "colab_type": "text"
      },
      "source": [
        "feature_engineering.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KKfBnhU1YDA",
        "colab_type": "code",
        "outputId": "bc2e3f60-2431-437d-c784-9863fe9a516d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "\"\"\"\n",
        "Detecting duplicate quora questions\n",
        "feature engineering\n",
        "@author: Abhishek Thakur\n",
        "\"\"\"\n",
        "\n",
        "import gc\n",
        "import pickle as cPickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from fuzzywuzzy import fuzz\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
        "from nltk import word_tokenize\n",
        "from nltk import download as nltk_download\n",
        "nltk_download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "\n",
        "def wmd(s1, s2):\n",
        "    s1 = str(s1).lower().split()\n",
        "    s2 = str(s2).lower().split()\n",
        "    stop_words = stopwords.words('english')\n",
        "    s1 = [w for w in s1 if w not in stop_words]\n",
        "    s2 = [w for w in s2 if w not in stop_words]\n",
        "    return model.wmdistance(s1, s2)\n",
        "\n",
        "\n",
        "def norm_wmd(s1, s2):\n",
        "    s1 = str(s1).lower().split()\n",
        "    s2 = str(s2).lower().split()\n",
        "    stop_words = stopwords.words('english')\n",
        "    s1 = [w for w in s1 if w not in stop_words]\n",
        "    s2 = [w for w in s2 if w not in stop_words]\n",
        "    return norm_model.wmdistance(s1, s2)\n",
        "\n",
        "\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower().decode('utf-8')\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(model[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    return v / np.sqrt((v ** 2).sum())\n",
        "\n",
        "gc.enable()\n",
        "\n",
        "data = pd.read_csv('/tmp/data/quora_duplicate_questions.tsv', sep='\\t')\n",
        "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
        "\n",
        "print('a')\n",
        "\n",
        "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
        "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
        "#data['diff_len'] = data.len_q1 - data.len_q2\n",
        "#data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "##data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "##data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
        "#data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
        "#data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
        "#data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "print('b')\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "print('b2')\n",
        "data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
        "print('gc experiment')\n",
        "model = None\n",
        "gc.collect()\n",
        "\n",
        "print('c')\n",
        "\n",
        "norm_model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "norm_model.init_sims(replace=True)\n",
        "print('c2')\n",
        "data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
        "norm_model = None\n",
        "gc.collect()\n",
        "\n",
        "print('d')\n",
        "\n",
        "question1_vectors = np.zeros((data.shape[0], 300))\n",
        "error_count = 0\n",
        "\n",
        "for i, q in tqdm(enumerate(data.question1.values)):\n",
        "    question1_vectors[i, :] = sent2vec(q)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "question2_vectors  = np.zeros((data.shape[0], 300))\n",
        "for i, q in tqdm(enumerate(data.question2.values)):\n",
        "    question2_vectors[i, :] = sent2vec(q)\n",
        "\n",
        "gc.collect()\n",
        "print('e')\n",
        "\n",
        "data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "                                                          np.nan_to_num(question2_vectors))]\n",
        "\n",
        "#data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "#                                                          np.nan_to_num(question2_vectors))]\n",
        "\n",
        "#data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "#                                                          np.nan_to_num(question2_vectors))]\n",
        "\n",
        "#data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "#                                                          np.nan_to_num(question2_vectors))]\n",
        "\n",
        "#data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "#                                                          np.nan_to_num(question2_vectors))]\n",
        "\n",
        "#data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "#                                                          np.nan_to_num(question2_vectors))]\n",
        "\n",
        "#data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
        "#                                                          np.nan_to_num(question2_vectors))]\n",
        "print('f')\n",
        "\n",
        "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
        "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
        "#data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
        "#data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
        "\n",
        "print('g')\n",
        "\n",
        "cPickle.dump(question1_vectors, open('/tmp/data/q1_w2v.pkl', 'wb'), -1)\n",
        "cPickle.dump(question2_vectors, open('/tmp/data/q2_w2v.pkl', 'wb'), -1)\n",
        "\n",
        "print('h')\n",
        "\n",
        "data.to_csv('/tmp/data/quora_features.csv', index=False)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "a\n",
            "b\n",
            "b2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:28: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gc experiment\n",
            "c\n",
            "c2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:36: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:37: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "d\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in double_scalars\n",
            "404290it [01:56, 3477.35it/s]\n",
            "404290it [02:00, 3342.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "e\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/scipy/spatial/distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "f\n",
            "g\n",
            "h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ljkr5St3GJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv('/content/gdrive/My Drive/Colab Notebooks/plagiarism_data/quora_features.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsZm11Uz-hH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmTVo4gy-AOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}